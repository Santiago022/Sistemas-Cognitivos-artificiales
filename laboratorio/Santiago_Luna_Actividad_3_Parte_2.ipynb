{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Santiago_Luna_Actividad_3_Parte_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mX8gZlVyCCbz"
      },
      "source": [
        "# Segunda parte: Modelos del lenguaje con RNNs\n",
        "\n",
        "En esta parte, vamos a entrenar un modelo del lenguaje basado en caracteres con Recurrent Neural Networks. Asimismo, utilizaremos el modelo para generar texto. En particular, alimentaremos nuestro modelo con obras de la literatura clásica en castellano para obtener una red neuronal que sea capaz de \"escribir\" fragmentos literarios.\n",
        "\n",
        "Los entrenamientos para obtener un modelo de calidad podrían tomar cierto tiempo (5-10 minutos por epoch), por lo que se aconseja empezar a trabajar pronto. El uso de GPUs no ayuda tanto con LSTMs como con CNNs, por lo que si tenéis máquinas potentes en casa es posible que podáis entrenar más rápido o a la misma velocidad que en Colab. En todo caso, la potencia de Colab es más que suficiente para completar esta actividad con éxito.\n",
        "\n",
        "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/d/d8/El_ingenioso_hidalgo_don_Quijote_de_la_Mancha.jpg\" style=\"text-align: center\" height=\"300px\"></center>\n",
        "\n",
        "El dataset a utilizar consistirá en un archivo de texto con el contenido íntegro en castellano antiguo de El Ingenioso Hidalgo Don Quijote de la Mancha, disponible de manera libre en la página de [Project Gutenberg](https://www.gutenberg.org). Asimismo, como apartado optativo en este laboratorio se pueden utilizar otras fuentes de texto. Aquí podéis descargar los datos a utilizar de El Quijote y un par de obras adicionales:\n",
        "\n",
        "[El ingenioso hidalgo Don Quijote de la Mancha (Miguel de Cervantes)](https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219424&authkey=AH0gb-qSo5Xd7Io)\n",
        "\n",
        "[Compilación de obras teatrales (Calderón de la Barca)](https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219433&authkey=AKvGD6DC3IRBqmc)\n",
        "\n",
        "[Trafalgar (Benito Pérez Galdós)](https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219434&authkey=AErPCAtMKOI5tYQ)\n",
        "\n",
        "Como ya deberíamos de estar acostumbrados en problemas de Machine Learning, es importante echar un vistazo a los datos antes de empezar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_PUXYU5RhUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6z_p9bqRhpz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QI274F8LQC59"
      },
      "source": [
        "## 1. Carga y procesado del texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MZNnzvXuqVVm"
      },
      "source": [
        "Primero, vamos a descargar el libro e inspeccionar los datos. El fichero a descargar es una versión en .txt del libro de Don Quijote, a la cual se le han borrado introducciones, licencias y otras secciones para dejarlo con el contenido real de la novela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D7tKOZ9BFfki",
        "outputId": "89ba149e-3e03-4e8f-f909-bf27721bd8ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "import numpy as np \n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "import random\n",
        "import io\n",
        "\n",
        "path = keras.utils.get_file(\n",
        "    fname=\"don_quijote.txt\", \n",
        "    origin=\"https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219424&authkey=AH0gb-qSo5Xd7Io\"\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219424&authkey=AH0gb-qSo5Xd7Io\n",
            "2154496/2151176 [==============================] - 1s 1us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VYGLvjLXrUUd"
      },
      "source": [
        "Una vez descargado, vamos a leer el contenido del fichero en una variable. Adicionalmente, convertiremos el contenido del texto a minúsculas para ponérselo un poco más fácil a nuestro modelo (de modo que todas las letras sean minúsculas y el modelo no necesite diferenciar entre minúsculas y mayúsculas).\n",
        "\n",
        "**1.1.** Leer todo el contenido del fichero en una única variable ***text*** y convertir el string a minúsculas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8WB6FejrrTu9",
        "colab": {}
      },
      "source": [
        "## TU CÓDIGO AQUÍ\n",
        "with open(path, encoding=\"utf8\") as f:\n",
        "    text = f.read().lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dkgGl8GWtUk8"
      },
      "source": [
        "Podemos comprobar ahora que efectivamente nuestra variable contiene el resultado deseado, con el comienzo tan característico del Quijote."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hMFhe3COFwSD",
        "outputId": "34fe40e7-d51a-4a8c-d50b-cc27255783b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "print(\"Longitud del texto: {}\".format(len(text)))\n",
        "print(text[0:300])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Longitud del texto: 2071198\n",
            "capítulo primero. que trata de la condición y ejercicio del famoso hidalgo\n",
            "don quijote de la mancha\n",
            "\n",
            "\n",
            "en un lugar de la mancha, de cuyo nombre no quiero acordarme, no ha mucho\n",
            "tiempo que vivía un hidalgo de los de lanza en astillero, adarga antigua,\n",
            "rocín flaco y galgo corredor. una olla de algo más\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bZ7TUXWiyvOj"
      },
      "source": [
        "## 2. Procesado de los datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x66_Vi_Gyxns"
      },
      "source": [
        "Una de las grandes ventajas de trabajar con modelos que utilizan caracteres en vez de palabras es que no necesitamos tokenizar el texto (partirlo palabra a palabra). Nuestro modelo funcionará directamente con los caracteres en el texto, incluyendo espacios, saltos de línea, etc.\n",
        "\n",
        "Antes de hacer nada, necesitamos procesar el texto en entradas y salidas compatibles con nuestro modelo. Como sabemos, un modelo del lenguaje con RNNs acepta una serie de caracteres y predice el siguiente carácter en la secuencia.\n",
        "\n",
        "* \"*El ingenioso don Qui*\" -> predicción: **j**\n",
        "* \"*El ingenioso don Quij*\" -> predicción: **o**\n",
        "\n",
        "De modo que la entrada y la salida de nuestro modelo necesita ser algo parecido a este esquema. En este punto, podríamos usar dos formas de preparar los datos para nuestro modelo.\n",
        "\n",
        "1. **Secuencia a secuencia**. La entrada de nuestro modelo sería una secuencia y la salida sería esa secuencia trasladada un caracter a la derecha, de modo que en cada instante de tiempo la RNN tiene que predecir el carácter siguiente. Por ejemplo:\n",
        "\n",
        ">* *Input*:   El ingenioso don Quijot \n",
        ">* *Output*: l ingenioso don Quijote\n",
        "\n",
        "2. **Secuencia a carácter**. En este variante, pasaríamos una secuencia de caracteres por nuestra RNN y, al llegar al final de la secuencia, predeciríamos el siguiente carácter.\n",
        "\n",
        ">* *Input*:   El ingenioso don Quijot \n",
        ">* *Output*: e\n",
        "\n",
        "En este laboratorio, por simplicidad, vamos a utilizar la segunda variante.\n",
        "\n",
        "De este modo, a partir del texto, hemos de generar nuestro propio training data que consista en secuencias de caracteres con el siguiente carácter a predecir. Para estandarizar las cosas, utilizaremos secuencias de tamaño *SEQ_LENGTH* caracteres (un hiperparámetro que podemos elegir nosotros).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mkfJUIxW5m5C"
      },
      "source": [
        "#### 2.1. Obtención de los caracteres y mapas de caracteres\n",
        "\n",
        "Antes que nada, necesitamos saber qué caracteres aparecen en el texto, ya que tendremos que diferenciarlos mediante un índice de 0 a *num_chars* - 1 en el modelo. Obtener:\n",
        " \n",
        "\n",
        "1.   Número de caracteres únicos que aparecen en el texto.\n",
        "2.   Diccionario que asocia char a índice único entre 0 y *num_chars* - 1. Por ejemplo, {'a': 0, 'b': 1, ...}\n",
        "3.   Diccionario reverso de índices a caracteres: {0: 'a', 1: 'b', ...}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAFv9BfUKU51",
        "colab_type": "code",
        "outputId": "5266dc36-ccec-4335-837c-3ad8a897fb68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "text[0:100]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'capítulo primero. que trata de la condición y ejercicio del famoso hidalgo\\ndon quijote de la mancha\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5bJ0NsbCbupF",
        "colab": {}
      },
      "source": [
        "## TU CÓDIGO AQUÍ\n",
        "chars=sorted(set(text))\n",
        "char_indices = dict((chars[index],index) for index in range(len(chars)))\n",
        "indice_char = {v: k for k, v in char_indices.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmtdffREKU67",
        "colab_type": "code",
        "outputId": "335c2fd1-717c-4152-9c97-0ade5cf9ce27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "print(len(chars))\n",
        "print(indice_char)\n",
        "print(char_indices)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "61\n",
            "{0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: \"'\", 5: '(', 6: ')', 7: ',', 8: '-', 9: '.', 10: '0', 11: '1', 12: '2', 13: '3', 14: '4', 15: '5', 16: '6', 17: '7', 18: ':', 19: ';', 20: '?', 21: ']', 22: 'a', 23: 'b', 24: 'c', 25: 'd', 26: 'e', 27: 'f', 28: 'g', 29: 'h', 30: 'i', 31: 'j', 32: 'l', 33: 'm', 34: 'n', 35: 'o', 36: 'p', 37: 'q', 38: 'r', 39: 's', 40: 't', 41: 'u', 42: 'v', 43: 'w', 44: 'x', 45: 'y', 46: 'z', 47: '¡', 48: '«', 49: '»', 50: '¿', 51: 'à', 52: 'á', 53: 'é', 54: 'í', 55: 'ï', 56: 'ñ', 57: 'ó', 58: 'ù', 59: 'ú', 60: 'ü'}\n",
            "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, \"'\": 4, '(': 5, ')': 6, ',': 7, '-': 8, '.': 9, '0': 10, '1': 11, '2': 12, '3': 13, '4': 14, '5': 15, '6': 16, '7': 17, ':': 18, ';': 19, '?': 20, ']': 21, 'a': 22, 'b': 23, 'c': 24, 'd': 25, 'e': 26, 'f': 27, 'g': 28, 'h': 29, 'i': 30, 'j': 31, 'l': 32, 'm': 33, 'n': 34, 'o': 35, 'p': 36, 'q': 37, 'r': 38, 's': 39, 't': 40, 'u': 41, 'v': 42, 'w': 43, 'x': 44, 'y': 45, 'z': 46, '¡': 47, '«': 48, '»': 49, '¿': 50, 'à': 51, 'á': 52, 'é': 53, 'í': 54, 'ï': 55, 'ñ': 56, 'ó': 57, 'ù': 58, 'ú': 59, 'ü': 60}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y_B4AWo0ElwA"
      },
      "source": [
        "#### 2.2. Obtención de secuencias de entrada y carácter a predecir\n",
        "\n",
        "Ahora, vamos a obtener las secuencias de entrada en formato texto y los correspondientes caracteres a predecir. Para ello, recorrer el texto completo leído anteriormente, obteniendo una secuencia de SEQ_LENGTH caracteres y el siguiente caracter a predecir. Una vez hecho, desplazarse un carácter a la izquierda y hacer lo mismo para obtener una nueva secuencia y predicción. Guardar las secuencias en una variable ***sequences*** y los caracteres a predecir en una variable ***next_chars***.\n",
        "\n",
        "Por ejemplo, si el texto fuera \"Don Quijote\" y SEQ_LENGTH fuese 5, tendríamos\n",
        "\n",
        "* *sequences* = [\"Don Q\", \"on Qu\", \"n Qui\", \" Quij\", \"Quijo\", \"uijot\"]\n",
        "* *next_chars* = ['u', 'i', 'j', 'o', 't', 'e']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfkuHqzTKU7y",
        "colab_type": "code",
        "outputId": "41f6adee-cdba-450d-853e-26ed1670617c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#print(text[0:33])\n",
        "print(text[0:30])\n",
        "print(text[30])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "capítulo primero. que trata de\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7--rZ043KU8N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pru =[]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "of9rX7CDKU9i",
        "colab_type": "code",
        "outputId": "04bb5d4c-da2a-42ea-a642-23a7ef0d2f48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(len(pru))\n",
        "pru.append(text[30])\n",
        "print(pru)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "[' ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NslxhnnDK6uA",
        "colab": {}
      },
      "source": [
        "# Definimos el tamaño de las secuencias. Puedes dejar este valor por defecto.\n",
        "SEQ_LENGTH = 30\n",
        "\n",
        "sequences = []\n",
        "next_chars = []\n",
        "\n",
        "## TU CÓDIGO AQUÍ\n",
        "# Realizar un for de 0 hasta tamaño len - seq_length.\n",
        "for i in range(0,len(text)-SEQ_LENGTH):\n",
        "    sequences.append(text[i:i+SEQ_LENGTH])\n",
        "    next_chars.append(text[i+SEQ_LENGTH])\n",
        "# Llenar sequences y next_chars .append..\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eCrLevAKU-d",
        "colab_type": "code",
        "outputId": "9d245a8e-b0ff-452f-b45a-ec7221415380",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "print(sequences[0:7])\n",
        "print(next_chars[0:7])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['capítulo primero. que trata de', 'apítulo primero. que trata de ', 'pítulo primero. que trata de l', 'ítulo primero. que trata de la', 'tulo primero. que trata de la ', 'ulo primero. que trata de la c', 'lo primero. que trata de la co']\n",
            "[' ', 'l', 'a', ' ', 'c', 'o', 'n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1Y3AmjYtHdLJ"
      },
      "source": [
        "Indicar el tamaño del training set que acabamos de generar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WVWqKxFcbwTu",
        "outputId": "13806b24-9afb-45fc-8f8d-f903c4729373",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "## TU CÓDIGO AQUÍ\n",
        "print(\"Tamaño de sequencias : \",len(sequences),\"Tamaño de los siguientes caracteres : \",len(next_chars))\n",
        "# Tamaño sequences"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tamaño de sequencias :  2071168 Tamaño de los siguientes caracteres :  2071168\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "goGQkKcwpLRJ"
      },
      "source": [
        "Como el Quijote es muy largo y tenemos muchas secuencias, podríamos encontrar problemas de memoria. Por ello, vamos a elegir un número máximo de ellas. Si estás corriendo esto localmente y tienes problemas de memoria, puedes reducir el tamaño aún más, pero ten cuidado porque, a menos datos, peor calidad del modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gF_IGFEkKU_i",
        "colab_type": "code",
        "outputId": "1c0ba26e-1106-4982-995e-90eff431d570",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "MAX_SEQUENCES = 500000\n",
        "\n",
        "perm = np.random.permutation(len(sequences)) #Permutar aleatoriamente una secuencia, o devolver un rango permutado.\n",
        "sequences, next_chars = np.array(sequences), np.array(next_chars) \n",
        "sequences, next_chars = sequences[perm], next_chars[perm]\n",
        "sequences, next_chars = list(sequences[:MAX_SEQUENCES]), list(next_chars[:MAX_SEQUENCES])\n",
        "\n",
        "print(len(sequences))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4FzgtAbPIs6f"
      },
      "source": [
        "#### 2.3. Obtención de input X y output y para el modelo\n",
        "\n",
        "Finalmente, a partir de los datos de entrenamiento que hemos generado vamos a crear los arrays de datos X e y que pasaremos a nuestro modelo.\n",
        "\n",
        "Para ello, vamos a utilizar *one-hot encoding* para nuestros caracteres. Por ejemplo, si sólo tuviéramos 4 caracteres (a, b, c, d), las representaciones serían: (1, 0, 0, 0), (0, 1, 0, 0), (0, 0, 1, 0) y (0, 0, 0, 1).\n",
        "\n",
        "De este modo, **X** tendrá shape *(num_sequences, seq_length, num_chars)* e **y** tendrá shape *(num_sequences, num_chars)*. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zMBwZ9obNGNg",
        "outputId": "91f793d5-c854-4438-f7fa-115a6305f41c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "NUM_CHARS = len(chars)  # Tu número de caracteres distintos aquí. Lo hecho en el 2.1\n",
        "NUM_SEQUENCES = len(sequences)\n",
        "X = np.zeros((NUM_SEQUENCES, SEQ_LENGTH, NUM_CHARS)) \n",
        "y = np.zeros((NUM_SEQUENCES, NUM_CHARS))\n",
        "\n",
        "## Tu código para rellenar X e y aquí. Pista: utilizar el diccionario de\n",
        "## chars a índices obtenido anteriormente junto con numpy. Por ejemplo,\n",
        "## si hacemos \n",
        "##     X[0, 1, char_to_indices['a']] = 1\n",
        "## estamos diciendo que para la segunda posición de la primera secuencia se\n",
        "## tiene una 'a'\n",
        "\n",
        "## TU CÓDIGO AQUÍ\n",
        "for i in range(len(sequences)):\n",
        "    for j in range (SEQ_LENGTH):\n",
        "        X[i, j, char_indices[sequences[i][j]]] = 1\n",
        "    y[i,char_indices[next_chars[i]]] = 1\n",
        "print(sequences[0][0])\n",
        "print(char_indices[sequences[0][0]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " \n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tW1O5AUoKVAT",
        "colab_type": "code",
        "outputId": "56d60f5b-d550-4a12-dc30-7dbdeca3b3f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "print(X[0,0,:])\n",
        "print(y[0,:])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IxeUxz3HPm3l"
      },
      "source": [
        "## 3. Definición del modelo y entrenamiento\n",
        "\n",
        "Una vez tenemos ya todo preparado, es hora de definir el modelo. Define un modelo que utilice una **LSTM** con **128 unidades internas**. Si bien el modelo puede definirse de una manera más compleja, para empezar debería bastar con una LSTM más una capa Dense con el *softmax* que predice el siguiente caracter a producir. Adam puede ser una buena elección de optimizador.\n",
        "\n",
        "Una vez el modelo esté definido, entrénalo un poco para asegurarte de que la loss es decreciente. No es necesario guardar la salida de este entrenamiento en el entregable final, ya que vamos a hacer el entrenamiento más informativo en el siguiente punto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MSw2j0btYWZs",
        "outputId": "94b791fb-9be0-4a7d-b1bd-f2a3b23527f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        }
      },
      "source": [
        "## TU CÓDIGO AQUÍ\n",
        "model = keras.Sequential()\n",
        "\n",
        "model.add(keras.layers.LSTM(256,return_sequences=True,input_shape=(SEQ_LENGTH, NUM_CHARS)))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.LSTM(180,return_sequences=True))\n",
        "model.add(keras.layers.Dropout(0.2))\n",
        "model.add(keras.layers.LSTM(100))\n",
        "model.add(keras.layers.Dropout(0.5))\n",
        "#model.add(keras.layers.GlobalAveragePooling1D())\n",
        "model.add(keras.layers.Dense(NUM_CHARS, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_20\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_28 (LSTM)               (None, 30, 256)           325632    \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 30, 256)           1024      \n",
            "_________________________________________________________________\n",
            "lstm_29 (LSTM)               (None, 30, 180)           314640    \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 30, 180)           0         \n",
            "_________________________________________________________________\n",
            "lstm_30 (LSTM)               (None, 100)               112400    \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 61)                6161      \n",
            "=================================================================\n",
            "Total params: 759,857\n",
            "Trainable params: 759,345\n",
            "Non-trainable params: 512\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MARwg_9DKVBK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', \n",
        "             loss='categorical_crossentropy', #la misma función de pérdida que CategoricalCrossentropy pero para problemas de dos clases\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAY6tO4KKVBS",
        "colab_type": "code",
        "outputId": "e951d522-aabe-411d-b501-45eb0425f6af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(X,\n",
        "                   y,\n",
        "                   epochs=40,\n",
        "                   batch_size=1024*16)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "500000/500000 [==============================] - 29s 59us/step - loss: 3.3657 - accuracy: 0.1277\n",
            "Epoch 2/40\n",
            "500000/500000 [==============================] - 28s 57us/step - loss: 3.0042 - accuracy: 0.1924\n",
            "Epoch 3/40\n",
            "500000/500000 [==============================] - 28s 57us/step - loss: 2.6397 - accuracy: 0.2648\n",
            "Epoch 4/40\n",
            "500000/500000 [==============================] - 28s 57us/step - loss: 2.4306 - accuracy: 0.2949\n",
            "Epoch 5/40\n",
            "500000/500000 [==============================] - 28s 57us/step - loss: 2.3299 - accuracy: 0.3151\n",
            "Epoch 6/40\n",
            "500000/500000 [==============================] - 28s 57us/step - loss: 2.2597 - accuracy: 0.3294\n",
            "Epoch 7/40\n",
            "500000/500000 [==============================] - 28s 57us/step - loss: 2.2044 - accuracy: 0.3422\n",
            "Epoch 8/40\n",
            "500000/500000 [==============================] - 28s 57us/step - loss: 2.1548 - accuracy: 0.3549\n",
            "Epoch 9/40\n",
            "500000/500000 [==============================] - 28s 57us/step - loss: 2.1119 - accuracy: 0.3660\n",
            "Epoch 10/40\n",
            "500000/500000 [==============================] - 28s 57us/step - loss: 2.0756 - accuracy: 0.3750\n",
            "Epoch 11/40\n",
            "500000/500000 [==============================] - 28s 57us/step - loss: 2.0435 - accuracy: 0.3841\n",
            "Epoch 12/40\n",
            "500000/500000 [==============================] - 28s 57us/step - loss: 2.0138 - accuracy: 0.3929\n",
            "Epoch 13/40\n",
            "500000/500000 [==============================] - 28s 57us/step - loss: 1.9865 - accuracy: 0.4008\n",
            "Epoch 14/40\n",
            "500000/500000 [==============================] - 28s 57us/step - loss: 1.9601 - accuracy: 0.4079\n",
            "Epoch 15/40\n",
            "500000/500000 [==============================] - 29s 57us/step - loss: 1.9355 - accuracy: 0.4163\n",
            "Epoch 16/40\n",
            "500000/500000 [==============================] - 28s 57us/step - loss: 1.9140 - accuracy: 0.4234\n",
            "Epoch 17/40\n",
            "500000/500000 [==============================] - 28s 57us/step - loss: 1.8917 - accuracy: 0.4299\n",
            "Epoch 18/40\n",
            "500000/500000 [==============================] - 29s 57us/step - loss: 1.8709 - accuracy: 0.4362\n",
            "Epoch 19/40\n",
            "500000/500000 [==============================] - 28s 57us/step - loss: 1.8514 - accuracy: 0.4425\n",
            "Epoch 20/40\n",
            "500000/500000 [==============================] - 28s 57us/step - loss: 1.8331 - accuracy: 0.4482\n",
            "Epoch 21/40\n",
            "500000/500000 [==============================] - 29s 57us/step - loss: 1.8134 - accuracy: 0.4538\n",
            "Epoch 22/40\n",
            "500000/500000 [==============================] - 28s 57us/step - loss: 1.7966 - accuracy: 0.4593\n",
            "Epoch 23/40\n",
            "500000/500000 [==============================] - 28s 57us/step - loss: 1.7831 - accuracy: 0.4643\n",
            "Epoch 24/40\n",
            "500000/500000 [==============================] - 28s 57us/step - loss: 1.7638 - accuracy: 0.4693\n",
            "Epoch 25/40\n",
            "500000/500000 [==============================] - 28s 57us/step - loss: 1.7492 - accuracy: 0.4729\n",
            "Epoch 26/40\n",
            "500000/500000 [==============================] - 28s 57us/step - loss: 1.7355 - accuracy: 0.4776\n",
            "Epoch 27/40\n",
            "500000/500000 [==============================] - 28s 57us/step - loss: 1.7203 - accuracy: 0.4819\n",
            "Epoch 28/40\n",
            "500000/500000 [==============================] - 28s 57us/step - loss: 1.7066 - accuracy: 0.4867\n",
            "Epoch 29/40\n",
            "500000/500000 [==============================] - 28s 57us/step - loss: 1.6940 - accuracy: 0.4893\n",
            "Epoch 30/40\n",
            "500000/500000 [==============================] - 28s 56us/step - loss: 1.6816 - accuracy: 0.4931\n",
            "Epoch 31/40\n",
            "500000/500000 [==============================] - 28s 56us/step - loss: 1.6690 - accuracy: 0.4969\n",
            "Epoch 32/40\n",
            "500000/500000 [==============================] - 28s 56us/step - loss: 1.6577 - accuracy: 0.5005\n",
            "Epoch 33/40\n",
            "500000/500000 [==============================] - 28s 56us/step - loss: 1.6458 - accuracy: 0.5041\n",
            "Epoch 34/40\n",
            "500000/500000 [==============================] - 28s 56us/step - loss: 1.6362 - accuracy: 0.5068\n",
            "Epoch 35/40\n",
            "500000/500000 [==============================] - 28s 56us/step - loss: 1.6256 - accuracy: 0.5094\n",
            "Epoch 36/40\n",
            "500000/500000 [==============================] - 28s 56us/step - loss: 1.6156 - accuracy: 0.5124\n",
            "Epoch 37/40\n",
            "500000/500000 [==============================] - 28s 56us/step - loss: 1.6052 - accuracy: 0.5151\n",
            "Epoch 38/40\n",
            "500000/500000 [==============================] - 28s 56us/step - loss: 1.5962 - accuracy: 0.5174\n",
            "Epoch 39/40\n",
            "500000/500000 [==============================] - 28s 56us/step - loss: 1.5875 - accuracy: 0.5207\n",
            "Epoch 40/40\n",
            "500000/500000 [==============================] - 28s 56us/step - loss: 1.5795 - accuracy: 0.5224\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yG32KQ5ZKvzK",
        "colab_type": "code",
        "outputId": "03275d00-a8ae-412e-89b3-ae7c26c48d01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        }
      },
      "source": [
        "history_dict = history.history #A partir del objeto hystory lo guardamos para graficarlo\n",
        "history_dict.keys()\n",
        "#%matplotlib inline\n",
        "\n",
        "acc= history.history['accuracy']\n",
        "#val_acc = history.history['val_accuracy']\n",
        "loss=history.history['loss']\n",
        "#val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) +1)\n",
        "\n",
        "#bo is for blue dot\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "#b is for solid blue line\n",
        "#plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.clf()\n",
        "acc_values = history_dict['accuracy']\n",
        "#val_acc_values = history_dict['val_accuracy']\n",
        "\n",
        "#bo is for blue dot\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "#b is for solid blue line\n",
        "#plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3de7xVdZ3/8dcbFBHFC0JlHOBgUUaZgGdQnHKstGgwycpfKpVWE+ElGxsz+zlNjuXvUdZo42RTOJOa4aA1TT+6mqV2mS5yTDQhSSQUGM0jGl4AufiZP9Z342Kzzt77XPbZ++zzfj4e+7HX+q7L/pwFZ3/O9/td3+9SRGBmZlZuWKMDMDOz5uQEYWZmhZwgzMyskBOEmZkVcoIwM7NCThBmZlbICcJqJukHkk7v730bSdIaScfV4bwh6aVp+cuSPlHLvr34nHmSftTbOM0qkcdBtDZJT+dWRwHPAjvS+gcjYtHAR9U8JK0B/iYiftzP5w1gSkSs6q99JbUDfwT2jIjt/RGnWSV7NDoAq6+I2Le0XOnLUNIe/tKxZuH/j83BTUxDlKRjJa2T9DFJjwDXSDpQ0ncldUl6Ii235Y65XdLfpOUzJP1C0ufTvn+U9OZe7jtZ0s8kPSXpx5KukvT1buKuJcZPSfrvdL4fSRqb2/5uSQ9K2iDpogrX50hJj0ganis7SdI9aXmmpF9J+rOkhyV9UdKIbs51raRP59Y/mo75H0nvK9t3jqS7JD0paa2ki3Obf5be/yzpaUmzStc2d/zRkpZK2pjej6712vTwOo+RdE36GZ6Q9O3ctrmSlqWf4QFJs1P5Ls15ki4u/TtLak9Nbe+X9BBwayr/Rvp32Jj+j7wyd/zekv4p/XtuTP/H9pb0PUkfKvt57pF0UtHPat1zghjaXgSMASYB88n+P1yT1icCm4EvVjj+SGAlMBa4DPh3SerFvjcAdwAHARcD767wmbXEeBrwXuAFwAjgfABJU4F/Ted/cfq8NgpExG+AZ4DXl533hrS8Azgv/TyzgDcAZ1WImxTD7BTP8cAUoLz/4xngPcABwBzgTElvTduOSe8HRMS+EfGrsnOPAb4HXJl+tsuB70k6qOxn2O3aFKh2na8na7J8ZTrXFSmGmcDXgI+mn+EYYE1316PAXwGvAN6U1n9Adp1eAPwWyDeJfh44Ajia7P/xBcBzwHXAu0o7STocGE92bawnIsKvIfIi+0U9Li0fC2wFRlbYfxrwRG79drImKoAzgFW5baOAAF7Uk33Jvny2A6Ny278OfL3Gn6koxr/PrZ8F/DAt/wOwOLdtn3QNjuvm3J8GvpqWR5N9eU/qZt+/Bf4rtx7AS9PytcCn0/JXgc/k9ntZft+C834BuCItt6d998htPwP4RVp+N3BH2fG/As6odm16cp2Bg8m+iA8s2O8rpXgr/f9L6xeX/p1zP9shFWI4IO2zP1kC2wwcXrDfSOAJsn4dyBLJlwb6960VXq5BDG1dEbGltCJplKSvpCr7k2RNGgfkm1nKPFJaiIhNaXHfHu77YuDxXBnA2u4CrjHGR3LLm3IxvTh/7oh4BtjQ3WeR1RbeJmkv4G3AbyPiwRTHy1KzyyMpjv9HVpuoZpcYgAfLfr4jJd2WmnY2AgtqPG/p3A+WlT1I9tdzSXfXZhdVrvMEsn+zJwoOnQA8UGO8RXZeG0nDJX0mNVM9yfM1kbHpNbLos9L/6RuBd0kaBpxKVuOxHnKCGNrKb2H7O+DlwJERsR/PN2l012zUHx4GxkgalSubUGH/vsT4cP7c6TMP6m7niFhB9gX7ZnZtXoKsqeo+sr9S9wP+b29iIKtB5d0ALAEmRMT+wJdz5612y+H/kDUJ5U0E1tcQV7lK13kt2b/ZAQXHrQVe0s05nyGrPZa8qGCf/M94GjCXrBluf7JaRimGx4AtFT7rOmAeWdPfpihrjrPaOEFY3miyavufU3v2J+v9gekv8k7gYkkjJM0C3lKnGL8JnCDpNalD+RKq/w7cAHyY7AvyG2VxPAk8LelQ4MwaY7gJOEPS1JSgyuMfTfbX+ZbUnn9ablsXWdPOId2c+/vAyySdJmkPSe8EpgLfrTG28jgKr3NEPEzWN/Cl1Jm9p6RSAvl34L2S3iBpmKTx6foALANOSft3AO+oIYZnyWp5o8hqaaUYniNrrrtc0otTbWNWqu2REsJzwD/h2kOvOUFY3heAvcn+Ovs18MMB+tx5ZB29G8ja/W8k+2Io0usYI2I5cDbZl/7DZO3U66oc9h9kHae3RsRjufLzyb68nwKuTjHXEsMP0s9wK7AqveedBVwi6SmyPpObcsduAi4F/lvZ3VNHlZ17A3AC2V//G8g6bU8oi7tW1a7zu4FtZLWoR8n6YIiIO8g6wa8ANgI/5flazSfI/uJ/AvhHdq2RFfkaWQ1uPbAixZF3PvA7YCnwOPBZdv1O+xpwGFmflvWCB8pZ05F0I3BfRNS9BmOtS9J7gPkR8ZpGxzJYuQZhDSfpLyS9JDVJzCZrd/52tePMupOa784CFjY6lsHMCcKawYvIbsF8muwe/jMj4q6GRmSDlqQ3kfXX/InqzVhWgZuYzMyskGsQZmZWqGUm6xs7dmy0t7c3Ogwzs0HlzjvvfCwixhVta5kE0d7eTmdnZ6PDMDMbVCSVj77fyU1MZmZWyAnCzMwKOUGYmVmhlumDKLJt2zbWrVvHli1bqu9s/WrkyJG0tbWx5557NjoUM+ullk4Q69atY/To0bS3t9P9c2ysv0UEGzZsYN26dUyePLnR4ZhZL7V0E9OWLVs46KCDnBwGmCQOOugg19zM6mzRImhvh2HDsvdFi6od0TMtnSAAJ4cG8XU367tKCWDRIpg/Hx58ECKy9/nz+zdJtHyCMDMbjKolgIsugk2bdj1m06asvL84QdTRhg0bmDZtGtOmTeNFL3oR48eP37m+devWisd2dnZy7rnnVv2Mo48+ur/CNbMBVqmGUC0BPPRQ8Tm7K+8NJ4ic/m7PO+igg1i2bBnLli1jwYIFnHfeeTvXR4wYwfbt27s9tqOjgyuvvLLqZ/zyl7/sW5BmVlfdfa9UqyFUSwATyx9WS+Xy3nCCSAaiPQ/gjDPOYMGCBRx55JFccMEF3HHHHcyaNYvp06dz9NFHs3LlSgBuv/12TjjhBAAuvvhi3ve+93HsscdyyCGH7JI49t133537H3vssbzjHe/g0EMPZd68eZRm6v3+97/PoYceyhFHHMG5556787x5a9as4bWvfS0zZsxgxowZuySez372sxx22GEcfvjhXHjhhQCsWrWK4447jsMPP5wZM2bwwAN9eU692eDV236CajWEagng0kth1Khdt40alZX3m4hoidcRRxwR5VasWLFbWXcmTYrI/gl3fU2aVPMpKvrkJz8Zn/vc5+L000+POXPmxPbt2yMiYuPGjbFt27aIiLjlllvibW97W0RE3HbbbTFnzpydx86aNSu2bNkSXV1dMWbMmNi6dWtEROyzzz47999vv/1i7dq1sWPHjjjqqKPi5z//eWzevDna2tpi9erVERFxyimn7Dxv3jPPPBObN2+OiIg//OEPUbqe3//+92PWrFnxzDPPRETEhg0bIiJi5syZ8a1vfSsiIjZv3rxze15Prr/ZYPT1r0eMGrXrd8aoUVl5ROXvFal4m1TbuUv7lM41adKu22oFdEY336uuQSQD0Z5XcvLJJzN8+HAANm7cyMknn8yrXvUqzjvvPJYvX154zJw5c9hrr70YO3YsL3jBC/jTn/602z4zZ86kra2NYcOGMW3aNNasWcN9993HIYccsnM8wqmnnlp4/m3btvGBD3yAww47jJNPPpkVK1YA8OMf/5j3vve9jEp/qowZM4annnqK9evXc9JJJwHZoLhR5X/KmLWIevUTVKshzJsHCxfCpEkgZe8LF2blJfPmwZo18Nxz2Xt+W39wgkgGoj2vZJ999tm5/IlPfILXve513HvvvXznO9/pduzAXnvttXN5+PDhhf0XtezTnSuuuIIXvvCF3H333XR2dlbtRDdrFX25lbQv/QS1NBHVOwFU4wSRDEh7XoGNGzcyfvx4AK699tp+P//LX/5yVq9ezZo1awC48cYbu43j4IMPZtiwYVx//fXs2LEDgOOPP55rrrmGTenPpMcff5zRo0fT1tbGt7+dPTb62Wef3bndrNn0JQHUs5+glhpCozlBJI36x7rgggv4+Mc/zvTp03v0F3+t9t57b770pS8xe/ZsjjjiCEaPHs3++++/235nnXUW1113HYcffjj33XffzlrO7NmzOfHEE+no6GDatGl8/vOfB+D666/nyiuv5NWvfjVHH300jzzySL/HbtZXfU0A1WoI1f6wrPa90ugaQlXddU70xwuYDawEVgEXFmw/g+zh4svS629y204H7k+v06t9Vl87qVvZU089FRERzz33XJx55plx+eWXD8jn+vrbQOmus7bazSfVOopruXmlPzqKG4kKndR1m6xP0nDgKuB4YB2wVNKSiFhRtuuNEXFO2bFjgE8CHUAAd6Zjn6hXvK3s6quv5rrrrmPr1q1Mnz6dD37wg40OyazflGoJpZpAqZYAtfURPFjwPLV8E1H+3FDcT9B0f/n3k3o2Mc0EVkXE6ojYCiwG5tZ47JuAWyLi8ZQUbiGrjVgvlAborVixgkWLFvmOIxt0ensnUV/HEgyGfoJ6qmeCGA+sza2vS2Xl3i7pHknflDShJ8dKmi+pU1JnV1dXYRBZDcoGmq+79Ze+3EnUHwmg6fsJ6qjRndTfAdoj4tVktYTrenJwRCyMiI6I6Bg3btxu20eOHMmGDRv8ZTXAIrLnQYwcObLRodgg0ZexBpVqCU4AfVPPBwatBybk1ttS2U4RsSG3+m/AZbljjy079vaeBtDW1sa6devornZh9VN6opxZNZX6EObNq+1Ookr9BK3cR1B33fVe9/VFlnxWA5OBEcDdwCvL9jk4t3wS8Ou0PAb4I3Bgev0RGFPp84ruYjKz5lDpTp9qdwoNhTuJGolGTLUREduBc4Cbgd8DN0XEckmXSDox7XaupOWS7gbOJbvtlYh4HPgUsDS9LkllZtaE6jkaeTCMOG5VihZpn+/o6IjOzs5Gh2E25JQ3EUH2BV5q629vL76VdNKk7Mu82vbSZ1x00fNzGJVGIlvfSbozIjoKtzlBmFlfVPuCHzYsqzmUk7K/+KslGKuvSgmi0Xcxmdkg0V0zUl8fbDPUxxo0s3rexWRmLaLSnUYejdy6XIMwM6D3YxE8Grl1uQ/CzKr2A9TSj+BO5MHJfRBmVrfRyuDbTFuVE4TZEDAQYxGs9ThBmLWIetcQ3I8w9DhBmLUAj1a2enCCMBskXEOwgeYEYTYIuIZgjeAEYTYIuIZgjeAEYdZEejudhWsIVg+easOsSfRlOovSl70Hq1l/8khqsyZRaVbU7uYzcjOR9ZVHUps1iUp3IlVqRnIfgjVCXROEpNmSVkpaJenCCvu9XVJI6kjr7ZI2S1qWXl+uZ5xmA6HanUiezsKaTd0ShKThwFXAm4GpwKmSphbsNxr4MPCbsk0PRMS09FpQrzjN+lNfxip4OgtrNvWsQcwEVkXE6ojYCiwG5hbs9yngs8CWOsZiVnd9HavgZiRrNvVMEOOBtbn1dalsJ0kzgAkR8b2C4ydLukvSTyW9to5xmvWLvo5VADcjWXNpWCe1pGHA5cDfFWx+GJgYEdOBjwA3SNqv4BzzJXVK6uzq6qpvwGb0vpMZ3IRkg089E8R6YEJuvS2VlYwGXgXcLmkNcBSwRFJHRDwbERsAIuJO4AHgZeUfEBELI6IjIjrGjRtXpx/DLNMfncxuQrLBpG7jICTtAfwBeANZYlgKnBYRy7vZ/3bg/IjolDQOeDwidkg6BPg5cFhEPN7d53kchNVbpXEKa9ZUfyqbWTNqyDiIiNgOnAPcDPweuCkilku6RNKJVQ4/BrhH0jLgm8CCSsnBrL/0pQnJNQRrNR5JbZZUqwFUq0GYDUYeSW1WA49TMNuVE4RZ4iYks105QdiQ010/g8cpmO3K033bkFJpSu3uZkx1E5INVU4QNqRU6mcodTT7mQpmGTcxWcvp662qbkIyyzhBWEvp62hnM3ueE4S1FN+qatZ/nCBs0PFoZ7OB4U5qG1Qq3YU0b17WVFQ02rn8VlUnBLPqXIOwQcVNSGYDxwnCBhU3IZkNHDcx2aDiJiSzgeMahDWdSp3QbkIyGzhOENZUqo1jcBOS2cDx8yCsqfiZC2YDq2HPg5A0W9JKSaskXVhhv7dLCkkdubKPp+NWSnpTPeO0gdWXcQxmNnDqliAkDQeuAt4MTAVOlTS1YL/RwIeB3+TKpgKnAK8EZgNfSuezQc5TYZgNHvWsQcwEVkXE6ojYCiwG5hbs9yngs8CWXNlcYHFEPBsRfwRWpfPZIOdxDGaDRz0TxHhgbW59XSrbSdIMYEJEfK+nx9rg5HEMZoNHw+5ikjQMuBz4uz6cY76kTkmdXV1d/Rec9Zmf2mY2+NUzQawHJuTW21JZyWjgVcDtktYARwFLUkd1tWMBiIiFEdERER3jxo3r5/Cttyr1M7gJyWzwqOdI6qXAFEmTyb7cTwFOK22MiI3A2NK6pNuB8yOiU9Jm4AZJlwMvBqYAd9QxVutHfmqbWWuoW4KIiO2SzgFuBoYDX42I5ZIuATojYkmFY5dLuglYAWwHzo6IHfWK1fpXLf0MTghmzc8D5azfebCb2eDRsIFy1ro8X5JZ63OCsB7zfElmQ4ObmKzH3IRk1jrcxGT9yvMlmQ0NThBWqFIfg+dLMhsanCBsN9X6GNwJbTY0OEHYbqpNqOdOaLOhwZ3Utpthw7KaQzkpmx/JzFqHO6mtR9zHYGbgBDFkeaCbmVXjBDEEeaCbmdXCfRBDkAe6mVlJn/ogJL0lPdzHWoQHuplZLWr54n8ncL+kyyQdWu+ArP7cCW1mtaiaICLiXcB04AHgWkm/So/6HF336KzX3AltZn1VU9NRRDwJfBNYDBwMnAT8VtKH6hib9ZI7oc2sP1TtpJZ0IvBe4KXA14DrIuJRSaOAFRHRXuHY2cA/kz1R7t8i4jNl2xcAZwM7gKeB+RGxQlI78HtgZdr11xGxoFKc7qR+njuhzaxWlTqpa3nk6NuBKyLiZ/nCiNgk6f0VPnQ4cBVwPLAOWCppSUSsyO12Q0R8Oe1/InA5MDtteyAiptUQn5VxJ7SZ9YdampguBu4orUjaO/2FT0T8pMJxM4FVEbE6IraSNU/Nze+Qmq5K9gFa457bBnMntJn1h1oSxDeA/Aw8O1JZNeOBtbn1dalsF5LOlvQAcBlwbm7TZEl3SfqppNfW8HmWuBPazPpDLQlij1QDACAtj+ivACLiqoh4CfAx4O9T8cPAxIiYDnwEuEHSfuXHprupOiV1dnV19VdIg547oc2sP9SSILpS/wAAkuYCj9Vw3HpgQm69LZV1ZzHwVoCIeDYiNqTlO8lusX1Z+QERsTAiOiKiY9y4cTWE1Doq3cYKWTJYsyabfXXNGicHM+u5WjqpFwCLJH0REFmz0XtqOG4pMEXSZLLEcApwWn4HSVMi4v60Oge4P5WPAx6PiB2SDgGmAKtr+MwhoXQba+mZDaXbWMGJwMz6T9UEEREPAEdJ2jetP13LiSNiu6RzgJvJbnP9akQsl3QJ0BkRS4BzJB0HbAOeAE5Phx8DXCJpG1n/x4KIeLyHP1vLqvRAHycIM+svNU3WJ2kO8EpgZKksIi6pY1w9NpTGQfiBPmbWX/o6Wd+XyeZj+hBZE9PJwKR+jdB6xLexmtlAqKWT+uiIeA/wRET8IzCLgg5jGzi+jdXMBkItCWJLet8k6cVk/QUH1y8kK+nuTiXfxmpmA6GWu5i+I+kA4HPAb8lGO19d16is6p1KpZeZWb1U7KRODwo6KiJ+mdb3AkZGxMYBiq9mrdZJ7Qn3zGwg9LqTOiKeI5twr7T+bDMmh1bkCffMrNFq6YP4iaS3S1Ldo7GdfKeSmTVaLQnig2ST8z0r6UlJT0l6stpB1je+U8nMGq2WR46OjohhETEiIvZL67tNnGf9y3cqmVmj1TJQ7pii10AE1+o84Z6ZNbNabnP9aG55JNmDgO4EXl+XiIYIT7hnZs2uliamt+RexwOvIptYz/qg0oR7ZmbNoJZO6nLrgFf0dyBDjW9jNbNmV7WJSdK/8PyzoocB08hGVFsfTJxYPBDOt7GaWbOopQ8iPzx5O/AfEfHfdYpnyLj00l37IMC3sZpZc6klQXwT2BIROwAkDZc0KiI2VTnOKih1RF90UdasNHFilhzcQW1mzaKmkdTA3rn1vYEf13JySbMlrZS0StKFBdsXSPqdpGWSfiFpam7bx9NxKyW9qZbPG2x8G6uZNbNaEsTI/GNG0/KoCvsDWU2DbB6nNwNTgVPzCSC5ISIOi4hpwGXA5enYqWTPsH4lMBv4UjqfmZkNkFoSxDOSZpRWJB0BbK7huJnAqohYHRFbgcXA3PwOEZGfsmMfnu8MnwssTpMD/hFYlc5nZmYDpJYE8bfANyT9XNIvgBuBc2o4bjywNre+LpXtQtLZkh4gq0Gc25Njm121kdJmZs2said1RCyVdCjw8lS0MiK29VcAEXEVcJWk04C/B06v9VhJ84H5ABOb7P5Qj5Q2s8GulrmYzgb2iYh7I+JeYF9JZ9Vw7vXAhNx6WyrrzmLgrT05NiIWRkRHRHSMGzeuhpAGjkdKm9lgV0sT0wci4s+llYh4AvhADcctBaZImixpBFmn85L8DpKm5FbnAPen5SXAKZL2kjQZmALcUcNnNg2PlDazwa6WcRDDJSnSs0nT3UQjqh0UEdslnQPcDAwHvhoRyyVdAnRGxBLgHEnHAdvI5nc6PR27XNJNwAqywXlnl8ZhDBYeKW1mg13FZ1IDSPocMAn4Sir6IPBQRJxf59h6pNmeSV3eBwHZSGk/08HMmkmvn0mdfAy4FViQXr9j14FzVsAP/DGzwa6Wu5iek/Qb4CXA/wHGAv9Z78Bawbx5TghmNnh1myAkvQw4Nb0eIxv/QES8bmBCMzOzRqpUg7gP+DlwQkSsApB03oBEZWZmDVepD+JtwMPAbZKulvQGQAMTlpmZNVq3CSIivh0RpwCHAreRTbnxAkn/KumNAxWgmZk1Ri3PpH4mIm6IiLeQjWi+i+zOJjMza2E9eiZ1RDyRprd4Q70CMjOz5tCjBGFmZkOHE0QfeDpvM2tltczFZAU8nbeZtTrXIHrJ03mbWatzguglT+dtZq3OCaKXupu229N5m1mrcILopUsvzabvzhs1Kis3M2sFThC95Om8zazV+S6mPvB03mbWyupag5A0W9JKSaskXViw/SOSVki6R9JPJE3KbdshaVl6LSk/1szM6qtuNYj07OqrgOOBdcBSSUsiYkVut7uAjojYJOlM4DLgnWnb5oiYVq/4zMyssnrWIGYCqyJidURsBRYDc/M7RMRtEVEaTfBrsskAzcysCdQzQYwH1ubW16Wy7rwf+EFufaSkTkm/lvTWogMkzU/7dHZ1dfU9YjMz26kpOqklvQvoAP4qVzwpItZLOgS4VdLvIuKB/HERsRBYCNDR0REDFrCZ2RBQzxrEemBCbr0tle1C0nHARcCJEfFsqTwi1qf31cDtwPQ6xmpmZmXqmSCWAlMkTZY0AjgF2OVuJEnTga+QJYdHc+UHStorLY8F/hLId26bmVmd1a2JKSK2SzoHuBkYDnw1IpZLugTojIglwOeAfYFvSAJ4KCJOBF4BfEXSc2RJ7DNldz+ZmVmdKaI1mu47Ojqis7Oz0WGYmQ0qku6MiI6ibZ5qw8zMCjlBmJlZIScIMzMr5ARhZmaFnCDMzKyQE4SZmRVygjAzs0JOEGZmVsgJwszMCjlBmJlZIScIMzMr5ARhZmaFnCDMzKyQE4SZmRVygjAzs0JOEGZmVqiuCULSbEkrJa2SdGHB9o9IWiHpHkk/kTQpt+10Sfen1+n1jNPMzHZXtwQhaThwFfBmYCpwqqSpZbvdBXRExKuBbwKXpWPHAJ8EjgRmAp+UdGC9YjUzs93VswYxE1gVEasjYiuwGJib3yEibouITWn110BbWn4TcEtEPB4RTwC3ALPrGKuZmZWpZ4IYD6zNra9LZd15P/CDnhwrab6kTkmdXV1dfQx3d4sWQXs7DBuWvS9a1O8fYWbWtPZodAAAkt4FdAB/1ZPjImIhsBCgo6Mj+jOmRYtg/nzYlOo3Dz6YrQPMm9efn2Rm1pzqWYNYD0zIrbelsl1IOg64CDgxIp7tybH1dNFFzyeHkk2bsnIzs6GgngliKTBF0mRJI4BTgCX5HSRNB75ClhwezW26GXijpANT5/QbU9mAeeihnpWbmbWauiWIiNgOnEP2xf574KaIWC7pEkknpt0+B+wLfEPSMklL0rGPA58iSzJLgUtS2YCZOLFn5WZmrUYR/dp03zAdHR3R2dnZb+cr74MAGDUKFi50H4SZtQ5Jd0ZER9E2j6Tuxrx5WTKYNAmk7N3JwcyGkqa4i6lZzZvnhGBmQ5drEGZmVsgJwszMCjlBmJlZIScIMzMr5ARhZmaFnCDMzKyQE4SZmRVygjAzs0JOEGZmVsgJwszMCjlBmJlZIScIMzMr5ARhZmaFnCDMzKxQXROEpNmSVkpaJenCgu3HSPqtpO2S3lG2bUd6ytzOJ82ZmdnAqdvzICQNB64CjgfWAUslLYmIFbndHgLOAM4vOMXmiJhWr/jMzKyyej4waCawKiJWA0haDMwFdiaIiFiTtj1XxzjMzKwX6tnENB5Ym1tfl8pqNVJSp6RfS3pr0Q6S5qd9Oru6uvoSq5mZlWnmTupJ6UHapwFfkPSS8h0iYmFEdEREx7hx4wY+QjOzFlbPBLEemJBbb0tlNYmI9el9NXA7ML0/gzMzs8rqmSCWAlMkTZY0AjgFqOluJEkHStorLY8F/pJc34WZmdVf3RJERGwHzgFuBn4P3BQRyyVdIulEAEl/IWkdcDLwFUnL0+GvADol3Q3cBnym7O4nMzOrM0VEo2PoFx0dHdHZ2dnoMMzMBhVJd6b+3t00cye1mZk1kBOEmZkVGvIJYtEiaG+HYcOy90WLGh2RmVlzqOdI6qa3aBm85d8AAAZ9SURBVBHMnw+bNmXrDz6YrQPMm9e4uMzMmsGQrkFcdNHzyaFk06as3MxsqBvSCeKhh3pWbmY2lAzpBDFxYs/KzcyGkiGdIC69FEaN2rVs1Kis3MxsqBvSCWLePFi4ECZNAil7X7jQHdRmZjDE72KCLBk4IZiZ7W5I1yDMzKx7ThBmZlbICcLMzAo5QZiZWSEnCDMzK9Qyz4OQ1AU8WGGXscBjAxROTzm23nFsvePYeqdVY5sUEeOKNrRMgqhGUmd3D8VoNMfWO46tdxxb7wzF2NzEZGZmhZwgzMys0FBKEAsbHUAFjq13HFvvOLbeGXKxDZk+CDMz65mhVIMwM7MecIIwM7NCLZ8gJM2WtFLSKkkXNjqecpLWSPqdpGWSOhscy1clPSrp3lzZGEm3SLo/vR/YRLFdLGl9unbLJP11A+KaIOk2SSskLZf04VTe8OtWIbZmuG4jJd0h6e4U2z+m8smSfpN+X2+UNKKJYrtW0h9z123aQMeWi3G4pLskfTet1+e6RUTLvoDhwAPAIcAI4G5gaqPjKotxDTC20XGkWI4BZgD35souAy5MyxcCn22i2C4Gzm/wNTsYmJGWRwN/AKY2w3WrEFszXDcB+6blPYHfAEcBNwGnpPIvA2c2UWzXAu9o5HXLxfgR4Abgu2m9Ltet1WsQM4FVEbE6IrYCi4G5DY6paUXEz4DHy4rnAtel5euAtw5oUEk3sTVcRDwcEb9Ny08BvwfG0wTXrUJsDReZp9PqnukVwOuBb6byRl237mJrCpLagDnAv6V1Uafr1uoJYjywNre+jib5BckJ4EeS7pQ0v9HBFHhhRDyclh8BXtjIYAqcI+me1ATVkOavEkntwHSyvzib6rqVxQZNcN1SM8ky4FHgFrLa/p8jYnvapWG/r+WxRUTpul2artsVkvZqRGzAF4ALgOfS+kHU6bq1eoIYDF4TETOANwNnSzqm0QF1J7L6a9P8JQX8K/ASYBrwMPBPjQpE0r7AfwJ/GxFP5rc1+roVxNYU1y0idkTENKCNrLZ/aCPiKFIem6RXAR8ni/EvgDHAxwY6LkknAI9GxJ0D8XmtniDWAxNy622prGlExPr0/ijwX2S/KM3kT5IOBkjvjzY4np0i4k/pF/k54GoadO0k7Un2BbwoIr6VipviuhXF1izXrSQi/gzcBswCDpBUehRyw39fc7HNTk12ERHPAtfQmOv2l8CJktaQNZm/Hvhn6nTdWj1BLAWmpB7+EcApwJIGx7STpH0kjS4tA28E7q181IBbApyelk8H/n8DY9lF6Qs4OYkGXLvU/vvvwO8j4vLcpoZft+5ia5LrNk7SAWl5b+B4sj6S24B3pN0add2KYrsvl/BF1sY/4NctIj4eEW0R0U72fXZrRMyjXtet0b3x9X4Bf01298YDwEWNjqcstkPI7qy6G1je6PiA/yBrcthG1o75frL2zZ8A9wM/BsY0UWzXA78D7iH7Qj64AXG9hqz56B5gWXr9dTNctwqxNcN1ezVwV4rhXuAfUvkhwB3AKuAbwF5NFNut6brdC3yddKdTo17AsTx/F1Ndrpun2jAzs0Kt3sRkZma95ARhZmaFnCDMzKyQE4SZmRVygjAzs0JOEGZVSNqRm8FzmfpxVmBJ7fkZas2ayR7VdzEb8jZHNu2C2ZDiGoRZLyl7lsdlyp7ncYekl6bydkm3pkndfiJpYip/oaT/Ss8ZuFvS0elUwyVdnZ498KM0ehdJ56ZnOdwjaXGDfkwbwpwgzKrbu6yJ6Z25bRsj4jDgi2SzbAL8C3BdRLwaWARcmcqvBH4aEYeTPdtieSqfAlwVEa8E/gy8PZVfCExP51lQrx/OrDseSW1WhaSnI2LfgvI1wOsjYnWaFO+RiDhI0mNk01dsS+UPR8RYSV1AW2STvZXO0U42nfSUtP4xYM+I+LSkHwJPA98Gvh3PP6PAbEC4BmHWN9HNck88m1vewfN9g3OAq8hqG0tzs3WaDQgnCLO+eWfu/Vdp+ZdkM20CzAN+npZ/ApwJOx9Is393J5U0DJgQEbeRPXdgf2C3WoxZPfkvErPq9k5PFyv5YUSUbnU9UNI9ZLWAU1PZh4BrJH0U6ALem8o/DCyU9H6ymsKZZDPUFhkOfD0lEQFXRvZsArMB4z4Is15KfRAdEfFYo2Mxqwc3MZmZWSHXIMzMrJBrEGZmVsgJwszMCjlBmJlZIScIMzMr5ARhZmaF/hfLol0TkcQLDQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3yUFHS4kHkyY"
      },
      "source": [
        "Para ver cómo evoluciona nuestro modelo del lenguaje, vamos a generar texto según va entrenando. Para ello, vamos a programar una función que, utilizando el modelo en su estado actual, genere texto, con la idea de ver cómo se va generando texto al entrenar cada epoch.\n",
        "\n",
        "En el código de abajo podemos ver una función auxiliar para obtener valores de una distribución multinomial. Esta función se usará para muestrear el siguiente carácter a utilizar según las probabilidades de la salida de softmax (en vez de tomar directamente el valor con la máxima probabilidad, obtenemos un valor aleatorio según la distribución de probabilidad dada por softmax, de modo que nuestros resultados serán más diversos, pero seguirán teniendo \"sentido\" ya que el modelo tenderá a seleccionar valores con más probabilidad).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LoGYpWOHd7Lr",
        "colab": {}
      },
      "source": [
        "def sample(probs, temperature=1.0):\n",
        "    \"\"\"Nos da el índice del elemento a elegir según la distribución\n",
        "    de probabilidad dada por probs.\n",
        "    \n",
        "    Args:\n",
        "      probs es la salida dada por una capa softmax:\n",
        "        probs = model.predict(x_to_predict)[0]\n",
        "      \n",
        "      temperature es un parámetro que nos permite obtener mayor\n",
        "        \"diversidad\" a la hora de obtener resultados. \n",
        "        \n",
        "        temperature = 1 nos da la distribución normal de softmax\n",
        "        0 < temperature < 1 hace que el sampling sea más conservador,\n",
        "          de modo que sampleamos cosas de las que estamos más seguros\n",
        "        temperature > 1 hace que los samplings sean más atrevidos,\n",
        "          eligiendo en más ocasiones clases con baja probabilidad.\n",
        "          Con esto, tenemos mayor diversidad pero se cometen más\n",
        "          errores.\n",
        "    \"\"\"\n",
        "    # Cast a float64 por motivos numéricos\n",
        "    probs = np.asarray(probs).astype('float64')\n",
        "    \n",
        "    # Hacemos logaritmo de probabilidades y aplicamos reducción\n",
        "    # por temperatura.\n",
        "    probs = np.log(probs) / temperature\n",
        "    \n",
        "    # Volvemos a aplicar exponencial y normalizamos de nuevo\n",
        "    exp_probs = np.exp(probs)\n",
        "    probs = exp_probs / np.sum(exp_probs)\n",
        "    \n",
        "    # Hacemos el sampling dadas las nuevas probabilidades\n",
        "    # de salida (ver doc. de np.random.multinomial)\n",
        "    samples = np.random.multinomial(1, probs, 1)\n",
        "    return np.argmax(samples)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_he_XdCRbG3",
        "colab_type": "code",
        "outputId": "5a7243bd-c817-4e46-b799-765f27977e9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "pru = [indice_char[sample(model.predict(X[i].reshape(1,30,61))[0])] for i in range(10)]\n",
        "print(pru)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[' ', 's', ' ', 's', 's', 'd', 'o', 'o', 'n', 'a']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3fejfZldd4ou"
      },
      "source": [
        "Utilizando la función anterior y el modelo entrenado, vamos a añadir un callback a nuestro modelo para que, según vaya entrenando, veamos los valores que resultan de generar textos con distintas temperaturas al acabar cada epoch.\n",
        "\n",
        "Para ello, abajo tenéis disponible el callback *on_epoch_end*. Esta función elige una secuencia de texto al azar en el texto disponible en la variable\n",
        "text y genera textos de longitud *GENERATED_TEXT_LENGTH* según las temperaturas en *TEMPERATURES_TO_TRY*, utilizando para ello la función *generate_text*.\n",
        "\n",
        "Completa la función *generate_text* de modo que utilicemos el modelo y la función sample para generar texto.\n",
        "\n",
        "NOTA: Cuando hagas model.predict, es aconsejable usar verbose=0 como argumento para evitar que la función imprima valores de salida."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xOEZvnBXkODd",
        "colab": {}
      },
      "source": [
        "TEMPERATURES_TO_TRY = [0.2, 0.5, 1.0, 1.2]\n",
        "GENERATED_TEXT_LENGTH = 300\n",
        "\n",
        "def generate_text(seed_text, model, length, temperature=1):\n",
        "    \"\"\"Genera una secuencia de texto a partir de seed_text utilizando model.\n",
        "    \n",
        "    La secuencia tiene longitud length y el sampling se hace con la temperature\n",
        "    definida.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Aquí guardaremos nuestro texto generado, que incluirá el\n",
        "    # texto origen\n",
        "    generated = seed_text\n",
        "    \n",
        "    # Utilizar el modelo en un bucle de manera que generemos\n",
        "    # carácter a carácter. Habrá que construir los valores de\n",
        "    # X_pred de manera similar a como hemos hecho arriba, salvo que\n",
        "    # aquí sólo se necesita una oración\n",
        "    # Nótese que el x que utilicemos tiene que irse actualizando con\n",
        "    # los caracteres que se van generando. La secuencia de entrada al\n",
        "    # modelo tiene que ser una secuencia de tamaño SEQ_LENGTH que\n",
        "    # incluya el último caracter predicho.\n",
        "    ### TU CÓDIGO AQUÍ\n",
        "\n",
        "    #print(len(generated))  #30\n",
        "    aux = \"\"\n",
        "    text_new = generated\n",
        "    for i in range(length):\n",
        "      new = np.zeros((1,len(text_new),NUM_CHARS))\n",
        "      for j in range (len(text_new)):\n",
        "        new[0,j,char_indices[text_new[j]]] = 1\n",
        "      char_new=indice_char[sample(model.predict(new,verbose=0)[0])]\n",
        "      text_new = text_new[1:] + char_new\n",
        "      aux += char_new\n",
        "    #print(new.shape) ##(1, 30, 61)\n",
        "    #print(char_new)\n",
        "    #print(\"Nuevo text : \",text_new)\n",
        "    #print(\"aux : \",aux)\n",
        "\n",
        "    ### FIN DE TU CÓDIGO\n",
        "    return generated + aux[1:]\n",
        "\n",
        "\n",
        "def on_epoch_end(epoch, logs):\n",
        "  print(\"\\n\\n\\n\")\n",
        "  \n",
        "  # Primero, seleccionamos una secuencia al azar para empezar a predecir\n",
        "  # a partir de ella\n",
        "  start_pos = random.randint(0, len(text) - SEQ_LENGTH - 1)\n",
        "  seed_text = text[start_pos:start_pos + SEQ_LENGTH]\n",
        "  for temperature in TEMPERATURES_TO_TRY:\n",
        "    print(\"------> Epoch: {} - Generando texto con temperature {}\".format(\n",
        "        epoch + 1, temperature))\n",
        "    \n",
        "    generated_text = generate_text(seed_text, model, \n",
        "                                   GENERATED_TEXT_LENGTH, temperature)\n",
        "    print(\"Seed: {}\".format(seed_text))\n",
        "    print(\"Texto generado: {}\".format(generated_text))\n",
        "    print()\n",
        "\n",
        "\n",
        "generation_callback = LambdaCallback(on_epoch_end=on_epoch_end)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-im1-l0WR_OA",
        "colab_type": "code",
        "outputId": "90711920-7300-4e41-e107-6e74f3b1ce87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 882
        }
      },
      "source": [
        "on_epoch_end(1,1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 2 - Generando texto con temperature 0.2\n",
            "Seed: l\n",
            "muchas serpientes, culebras \n",
            "Texto generado: l\n",
            "muchas serpientes, culebras ue fel y pojungo, que ey la que fueron a la lotar por entantos enegrecos;\n",
            "y por seuso;\n",
            "y coma.\n",
            "-no de esto que fino; don quijote, por lo de drego ji acabar, pués te cuerpo,\n",
            "se de que este\n",
            "so\n",
            "que duda acoda-, señor con su\n",
            "como se hijo, al entra: aventura de poernos con traer erranclinó de pensoso de\n",
            "\n",
            "------> Epoch: 2 - Generando texto con temperature 0.5\n",
            "Seed: l\n",
            "muchas serpientes, culebras \n",
            "Texto generado: l\n",
            "muchas serpientes, culebras elamos sé\n",
            "más deterlantes que al valto que diciendo las más te nuestros\n",
            "insiarnos espacíanos es un gardominos hablarque no es vodrió el ando el\n",
            "el guciene.\n",
            "\n",
            "obregunta don quijote, que moy renginamenera en la enga qaila de\n",
            "maltán de coníen mandahora el una portencia que los guas, grandes así se\n",
            "quec\n",
            "\n",
            "------> Epoch: 2 - Generando texto con temperature 1.0\n",
            "Seed: l\n",
            "muchas serpientes, culebras \n",
            "Texto generado: l\n",
            "muchas serpientes, culebras  deste combazo, y me imipudiere. dijo goantos,\n",
            "alginarensásén el loráncho él mesmo con la frán andante de leemor o todo sus trayon reves\n",
            "de cuando pedir la mí hasto en lo que quisere por sernas luces de sus caballos, esta eran -dijo tantole no que del dolantes y vestubados y decir el otro\n",
            "dieron. y\n",
            "\n",
            "------> Epoch: 2 - Generando texto con temperature 1.2\n",
            "Seed: l\n",
            "muchas serpientes, culebras \n",
            "Texto generado: l\n",
            "muchas serpientes, culebras  el costivo manees y cerginantos don\n",
            "languas tan oecidas raudas reves humonrando de haal y empondió tan mazua del curer de la señora, que a su hocirfante sin historias, son fueró pasado el encubridiera a la capa lingotes y las alicas, tenmrempreso, no me muquí de mido, que juliste don quijote-, no \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BSMYZ2JdrSJg"
      },
      "source": [
        "Entrena ahora tu modelo. No te olvides de añadir *generation_callback* a la lista de callbacks utilizados en fit(). Ya que las métricas de clasificación no son tan críticas aquí (no nos importa tanto acertar el carácter exacto, sino obtener una distribución de probabilidad adecuada), no es necesario monitorizar la accuracy ni usar validation data, si bien puedes añadirlos para asegurarte de que todo está en orden.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3oT7pNvjrP2e",
        "outputId": "da75255a-6ff4-4331-c181-e1dd015f1c98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## TU CÓDIGO AQUÍ\n",
        "\n",
        "model.fit(X,y,epochs=10,batch_size=1024*16,\n",
        "          callbacks=[generation_callback])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "500000/500000 [==============================] - 28s 56us/step - loss: 1.5695 - accuracy: 0.5253\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 1 - Generando texto con temperature 0.2\n",
            "Seed: a desesperado por la\n",
            "repentina\n",
            "Texto generado: a desesperado por la\n",
            "repentinaa, a quien vuestre\n",
            "leglo que en vido, que le digo del que los tates, y algando de mumbre para los días, en el vio, señor, sino señor lo que iyendo de liente\n",
            "que en el bustado en pensante\n",
            "o minarle que estabo en ujome curta y más fejra con arbezaz de su venis, de ducudir en el yos\n",
            "de pasadera de\n",
            "suj\n",
            "\n",
            "------> Epoch: 1 - Generando texto con temperature 0.5\n",
            "Seed: a desesperado por la\n",
            "repentina\n",
            "Texto generado: a desesperado por la\n",
            "repentinaa,\n",
            "   purciugo que juica, por dentracientos de bovateo, como bloroso este megoré su lela otal, aquialles verpaste bu-rasé, y de callar otran padres\n",
            "dentiras éste en el calela, y tóno le todo me ciento, dijo, que verro en ajarata la mucha aherdipa, yo con incundar\n",
            "por alguna liñay. griaro catía\n",
            "   s\n",
            "\n",
            "------> Epoch: 1 - Generando texto con temperature 1.0\n",
            "Seed: a desesperado por la\n",
            "repentina\n",
            "Texto generado: a desesperado por la\n",
            "repentinale a\n",
            "todo otro tu leche; que: sabe en\n",
            "qué\n",
            "sanzo\n",
            "se hallación, que me encanga, todo uno\n",
            "tan sercice entino mi aquel vedado teda posible estrevo de\n",
            "cuabro, son torgo\n",
            "de volver en hegnara cuvel cocado, que le ponere dejó vuestro fiente las señas andenvilladoros,,\n",
            "porque; con hasto dudada de los amosta\n",
            "\n",
            "------> Epoch: 1 - Generando texto con temperature 1.2\n",
            "Seed: a desesperado por la\n",
            "repentina\n",
            "Texto generado: a desesperado por la\n",
            "repentinalo; y\n",
            "echallaron y\n",
            "gicenestese hala vestrado a rotran, que arbatanto tesdero\n",
            "la los romas:\n",
            ":\n",
            "vábíonsale enciertos de natorse, y, con caconza del voces catienso de quisa pasado lucentos náechos y mulles en todo las pobres; pues. señor inperte respondersir de daje, hasí todos uno bollada y camisisino\n",
            "\n",
            "Epoch 2/10\n",
            "500000/500000 [==============================] - 28s 56us/step - loss: 1.5623 - accuracy: 0.5274\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 2 - Generando texto con temperature 0.2\n",
            "Seed:  de buscar algún lugar donde b\n",
            "Texto generado:  de buscar algún lugar donde bllencia las\n",
            "totosaciados, lo guca qué dejote\n",
            "maver, a este han la contenir cristiano estamál nuestro trienso, y con jurido rostro ¿los\n",
            "pies no le caballeres de entencito en los\n",
            "vopijas.\n",
            "\n",
            "-ya dijo, de\n",
            "mi mano éste\n",
            "se vida tiéndole yla; que en hocer\n",
            "saber gonces año,\n",
            "sancho ¿y estarque según vaba de \n",
            "\n",
            "------> Epoch: 2 - Generando texto con temperature 0.5\n",
            "Seed:  de buscar algún lugar donde b\n",
            "Texto generado:  de buscar algún lugar donde blquera, por omico de mucho, sé es mal ventero adello que calle se me el y a fuer, para haber lo que cielo, y en\n",
            "mended por alguna\n",
            "que crear a contra desmadente iblanto y don quijote. rebico, y,\n",
            "antes personas diradas. con, muchos bastas la mijodos que no quería que le cosa otra despetades la nervis\n",
            "\n",
            "------> Epoch: 2 - Generando texto con temperature 1.0\n",
            "Seed:  de buscar algún lugar donde b\n",
            "Texto generado:  de buscar algún lugar donde brbado libro, dicen, donde haa y\n",
            "artorja, altentigo, en cueno y desta él haber amor de gosto de nuerte y\n",
            "las buranse -respondia en este les entendió\n",
            "mentendos en consenta hay luces cosar a sancho a su dardo del listor tomar a\n",
            "tulcos, por vista la boza vaz como quesase, modiste mi sula que nuevas est\n",
            "\n",
            "------> Epoch: 2 - Generando texto con temperature 1.2\n",
            "Seed:  de buscar algún lugar donde b\n",
            "Texto generado:  de buscar algún lugar donde blgansea de tenían; cuando, décido que no estaña ser\n",
            "ducienza.\n",
            "\n",
            "perronando sancho carca de\n",
            "zaía pensancia y abritado\n",
            "del diis un luego, carrinco\n",
            "y veficado por año palimienmas no las mías monestedos, como esto es haciendo pasado y entrar esas de las negras de los vinos, que caminaban dolimer pueto d\n",
            "\n",
            "Epoch 3/10\n",
            "500000/500000 [==============================] - 28s 56us/step - loss: 1.5555 - accuracy: 0.5292\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 3 - Generando texto con temperature 0.2\n",
            "Seed: os ojos y vee las cosas\n",
            "debajo\n",
            "Texto generado: os ojos y vee las cosas\n",
            "debajo; no muchos que quearen contra preguntarse lavardanza, nos sabensarcos, y como el recon, la lagrando que digó bordante\n",
            "con su hijas con tras tar cosas, sí, vuestras fin servidos, medos, estonca don quijote, cuon el fuiso, verdadera debazánrés. estamíale por en todo\n",
            "a mí a endónde a unos que entondi\n",
            "\n",
            "------> Epoch: 3 - Generando texto con temperature 0.5\n",
            "Seed: os ojos y vee las cosas\n",
            "debajo\n",
            "Texto generado: os ojos y vee las cosas\n",
            "debajo de\n",
            "honres samigo que tan buenas que, me cual y sin són prensáenle la guara la cama., te,s en mío es orros contentos; y con la cupita y\n",
            "contra amor más de paricentes caballeres:\n",
            "\n",
            "-todo soy, no sán suermo caballero del atre lonago, samina libro que tal o. unó la ciucia nombre, pace no estar alegacha\n",
            "\n",
            "------> Epoch: 3 - Generando texto con temperature 1.0\n",
            "Seed: os ojos y vee las cosas\n",
            "debajo\n",
            "Texto generado: os ojos y vee las cosas\n",
            "debajo y tan donde la franga.\n",
            "   cuelarfación al oíño, y buen al masbertos otros. sillín, venerador de su vino tuerco, que pueda hacer el mundo para feter\n",
            "contar\n",
            "glitorias de la ladó odorró la grandipaipañes, tuenta de hodrante y milos espejos que\n",
            "impendaza más; y; y dice en vol ventó en almó de cabrigo \n",
            "\n",
            "------> Epoch: 3 - Generando texto con temperature 1.2\n",
            "Seed: os ojos y vee las cosas\n",
            "debajo\n",
            "Texto generado: os ojos y vee las cosas\n",
            "debajo se de duciendo, por las encírpares,\n",
            "señor que las pragues nos milgaron siémalas nuestros sobresenos havos pues de las negimesas aventantes que ise había agúnos, sandido las manos aun franción.\n",
            "   amistar a nancha con los sobres a favorcer de don quiisto de\n",
            "bien vencidos, vizsitan pongante, grandes\n",
            "\n",
            "Epoch 4/10\n",
            "500000/500000 [==============================] - 28s 56us/step - loss: 1.5462 - accuracy: 0.5318\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 4 - Generando texto con temperature 0.2\n",
            "Seed: enido más próspera ventura que\n",
            "Texto generado: enido más próspera ventura quedecía, porque de vieneñas, en gelos en la comta del por pesadoo, cuando lo gupierron señoros, no me conquiera acertada llegarda, que abraja no vuestra decirtan una ausenza que me vuestra bien alvamo en la que sodreta\n",
            "y tal pasamos de don quijote, todo el toboso con el sumesmo aquella de dul vierio,\n",
            "\n",
            "------> Epoch: 4 - Generando texto con temperature 0.5\n",
            "Seed: enido más próspera ventura que\n",
            "Texto generado: enido más próspera ventura quea mi un modo; y desta en la plara de todo respondiérentus, que señale del cesa l brico, pues, y con don quijote, respondió\n",
            "la\n",
            "erpeza tan grante; pero de presante con verten en otra barca que me memosido que guirlado llamago,\n",
            "las apristas lativos elsonos podías no hay aendo que dame contra lugar don\n",
            "\n",
            "------> Epoch: 4 - Generando texto con temperature 1.0\n",
            "Seed: enido más próspera ventura que\n",
            "Texto generado: enido más próspera ventura queal\n",
            "vez los\n",
            "cabeen llamarias y mastemía del berlea, y es señoa y pasó\n",
            "muchas pregalas y deseruablas lención de por vuestra había muchos.\n",
            "tan en tulela ser yl parte para gran que le pudiero que lo labéricísito, carredamento lutara pesada que lo\n",
            "viemmo don quién dee cuedamente, y\n",
            "volgo perdo contido, \n",
            "\n",
            "------> Epoch: 4 - Generando texto con temperature 1.2\n",
            "Seed: enido más próspera ventura que\n",
            "Texto generado: enido más próspera ventura quehistoria que ha ganta la yerististad, que le; y yo, no puemente por bien odos que    que leguas de deventa a la\n",
            "lanzó a\n",
            "teminas.\n",
            "\n",
            "celapulas en mi un todo respondiese que, princemas   nujito por intento y\n",
            "cieien:\n",
            "\n",
            "-si contencia no desta ¿puesto todos les\n",
            "sentan algunas curaos hacer con su aobra del \n",
            "\n",
            "Epoch 5/10\n",
            "500000/500000 [==============================] - 28s 56us/step - loss: 1.5395 - accuracy: 0.5339\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 5 - Generando texto con temperature 0.2\n",
            "Seed: cura, de quien ya\n",
            "iba aficiona\n",
            "Texto generado: cura, de quien ya\n",
            "iba aficiona tal bresto en él quesibno\n",
            "en configo que se seibra que está amamente rescornó de advirjeras, estenade, no quiesque de la verso.\n",
            " si había dicho, que todos hecha gener ciertas por tras buenas cartames.\n",
            "\n",
            "-quierá no a paseón\n",
            "ever quisé para de don quijote del puesto. pero dejara en a¿irete la zesería\n",
            "\n",
            "------> Epoch: 5 - Generando texto con temperature 0.5\n",
            "Seed: cura, de quien ya\n",
            "iba aficiona\n",
            "Texto generado: cura, de quien ya\n",
            "iba aficionate dios sancho por decel nuya tan como se istaver vengar.\n",
            "  nodro en el músoro\n",
            "ya necotraja en la cual se parece le desover un otras dos se cieridos\n",
            "sis\n",
            "cuves de mi íste heguos era yo despuesta que yo se seráí decéndre ni endio, con decir, sebarmante por qué laces.\n",
            "\n",
            "famalde tuarpanra que se propele\n",
            "\n",
            "------> Epoch: 5 - Generando texto con temperature 1.0\n",
            "Seed: cura, de quien ya\n",
            "iba aficiona\n",
            "Texto generado: cura, de quien ya\n",
            "iba aficionao el bocán, de tufen lo que yo me castel cual de que mesderesen el deseo de la vista a tener mejor la viese llana lo que este los embavaban, se lezan, si lo que todo maber al denejo, le\n",
            "diciendo\n",
            "   prirce.\n",
            "\n",
            "-que deja; uno la dejo don quijote: ¡ayó a! mestrón visto volazgicas la frina en ella femosu\n",
            "\n",
            "------> Epoch: 5 - Generando texto con temperature 1.2\n",
            "Seed: cura, de quien ya\n",
            "iba aficiona\n",
            "Texto generado: cura, de quien ya\n",
            "iba aficionao mostrar deseo esta cabeva, como no dijojes de ni volmisa escudieres, ¿como fue andes de\n",
            "la yerra, santas pincianas, por no rebulabos de él aquella la aviso\n",
            "don quijote.\n",
            "\n",
            "-él la\n",
            "hilló grancio, porque le halló que no ha que mivo gallo me\n",
            "don quijote serlamente;\n",
            "tigan\n",
            "quiere, que el gánes y estaba a\n",
            "\n",
            "Epoch 6/10\n",
            "500000/500000 [==============================] - 28s 56us/step - loss: 1.5313 - accuracy: 0.5360\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 6 - Generando texto con temperature 0.2\n",
            "Seed: amo; y, como no las halló, est\n",
            "Texto generado: amo; y, como no las halló, ests que al\n",
            "más ricote aelmonados caballaras que suelidos\n",
            "ni, dorando que una ventura que serido que men señompor en los abeteres menteros de su abragiero que a mundos que, que yaba anzado con dustar en lacendal; y la nrasa las mientos, memorevos como no noes fuera me rata de la  como venía del parcie\n",
            "\n",
            "------> Epoch: 6 - Generando texto con temperature 0.5\n",
            "Seed: amo; y, como no las halló, est\n",
            "Texto generado: amo; y, como no las halló, estmos sulabo modo,\n",
            "porque mijoldos que del yasen\n",
            "los\n",
            "ogoros, al tiegen sirtas en la jula, con quien mi enconcedor?\n",
            "\n",
            "-el esto\n",
            "puento; y arando yo dejado del pedo.\n",
            "   anmino de\n",
            "un santo está arja y, donde aquel deshora merced sus solo\n",
            "vasillas, baja del am-ridor con él pedro? -respondió don quijote-, y\n",
            "\n",
            "------> Epoch: 6 - Generando texto con temperature 1.0\n",
            "Seed: amo; y, como no las halló, est\n",
            "Texto generado: amo; y, como no las halló, estba la gente duquerima que. salhó a otra mareco, estando, entramones, suelidor la cuerca, algro\n",
            "algraras\n",
            "hisno, de\n",
            "tener que nuelo ausento; curulada\n",
            "ùlamá con amor hasta como perdona, y se bella de aventuró a los\n",
            "bastan por noticia, no horiendo, una de sharo yo quepiera el había tento contento y cab\n",
            "\n",
            "------> Epoch: 6 - Generando texto con temperature 1.2\n",
            "Seed: amo; y, como no las halló, est\n",
            "Texto generado: amo; y, como no las halló, estcbuendo tesón -dijo sancho-, que todo esta delanta; era al siqueron más y cubrillo, no ni decernanse bien osás que rejo en allí,\n",
            "que a nado estíco por quiere una\n",
            "yo pinces esperanza que vencera,\n",
            "volveré los\n",
            "evaleras una otro\n",
            "una hacuado echollado lo puede\n",
            "las que el sí, para que suspirugos sobre de\n",
            "\n",
            "Epoch 7/10\n",
            "500000/500000 [==============================] - 28s 56us/step - loss: 1.5244 - accuracy: 0.5380\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 7 - Generando texto con temperature 0.2\n",
            "Seed:  dos que\n",
            "hay, yo no conozco si\n",
            "Texto generado:  dos que\n",
            "hay, yo no conozco si\n",
            "grandezos y mondaron bien tallas casimillas ascos? lé mi tusponso penos pinímas, que no degan cantenios de ruderos de monto, y estaba en donde curarse suyer con los deseos tuvieron a techos\n",
            "teneros cortesos de\n",
            "la parecerle por discráltes, a yo por grantees herundado dejar de doraterse con los que \n",
            "\n",
            "------> Epoch: 7 - Generando texto con temperature 0.5\n",
            "Seed:  dos que\n",
            "hay, yo no conozco si\n",
            "Texto generado:  dos que\n",
            "hay, yo no conozco si\n",
            "truertos libros de\n",
            "laltan encabardaca, ni tan pobre más, \"mi\n",
            "bamos todos retiblos propes\n",
            "cenpos\n",
            "habierdelles, el que los clarellos, y dio a su groyo aquella se haque la ventera y asgidar nada precaos    no hapiciula y casadi! -respantamente y no higo, ventura con ella avortas y\n",
            "delsa\n",
            "helles, que d\n",
            "\n",
            "------> Epoch: 7 - Generando texto con temperature 1.0\n",
            "Seed:  dos que\n",
            "hay, yo no conozco si\n",
            "Texto generado:  dos que\n",
            "hay, yo no conozco sia mostraba y veda a otra\n",
            "tanta y puenta la\n",
            "cartas por no entría, para docinanza del zofroda la becado carza. habme que no\n",
            "estara nos suelas, a hallo los duchos de vinir los deneras, con los señoras, ni sin hacierto decía, finaldes y pasaciantes al cura de haber veñas de vsentas andades, fafose; san\n",
            "\n",
            "------> Epoch: 7 - Generando texto con temperature 1.2\n",
            "Seed:  dos que\n",
            "hay, yo no conozco si\n",
            "Texto generado:  dos que\n",
            "hay, yo no conozco si lugan, pero el ser y dejan y barral alcindo, vitiosa\n",
            "nuestro no ciele tormás y he bospazabo de quesa fresales, más que les escriaron tantos que estaba mismo ningudas penas cada valos, pues con que no me sayo por esperadada fuerte el corca\n",
            "gesto con aldar esta grandes; que\n",
            "descubrielonas lo que hle\n",
            "\n",
            "Epoch 8/10\n",
            "500000/500000 [==============================] - 28s 56us/step - loss: 1.5176 - accuracy: 0.5401\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 8 - Generando texto con temperature 0.2\n",
            "Seed: l duque mi marido, en una casa\n",
            "Texto generado: l duque mi marido, en una casaque tiene aun\n",
            "soñas de las todos de los habían de dedicen\n",
            "dello.\n",
            "  una mefrise; que yo en corazares en el pededo; pero trales salircunaces, que, pero se diese que, ciero, le tirio, ella estuya tiemposimientos leencias de\n",
            "descuros cangas\n",
            "naspallas,\n",
            "zalantes bles. eso hallaño una grande\n",
            "y faltarpa ca\n",
            "\n",
            "------> Epoch: 8 - Generando texto con temperature 0.5\n",
            "Seed: l duque mi marido, en una casa\n",
            "Texto generado: l duque mi marido, en una casa que la he nóle lleco, portunación y quiente, esta de decir a cuado dineros, a no una cual, o que, beyo me\n",
            "ando no y derazándole había ya rodación de su sácema con sus yten¿ressos hadas de márices que alía. por lle. miró polabre ditárdare a ristorñas en uneva con españa, decir uno de licestos diero\n",
            "\n",
            "------> Epoch: 8 - Generando texto con temperature 1.0\n",
            "Seed: l duque mi marido, en una casa\n",
            "Texto generado: l duque mi marido, en una casade la veste ella a, desto amao camino de otras principlersas,\n",
            "debenderido de cortes bien no sez desde, nos aentas había teneblo, el reesombrar la señora; y el bien de pontar dsis había que habées estad comér caballeros, lo no de varélos descurazan el mal derechar; si vuese proyado tengazaos y sanch\n",
            "\n",
            "------> Epoch: 8 - Generando texto con temperature 1.2\n",
            "Seed: l duque mi marido, en una casa\n",
            "Texto generado: l duque mi marido, en una casaesta quiere otro lebrador, sin la\n",
            "cosecía a codicieron lalpos todos\n",
            "que sobre este la dio había\n",
            "fisto, pues a dorotar mastrarme del rapo, adelante en verdad del que tan sacar mandaron que tanto, allero trainguenta abrazar o, sino los cuales le descucedios, y mirellos en auno quen\n",
            "sancho que han sie\n",
            "\n",
            "Epoch 9/10\n",
            "500000/500000 [==============================] - 28s 56us/step - loss: 1.5121 - accuracy: 0.5415\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 9 - Generando texto con temperature 0.2\n",
            "Seed: de la respuesta que de parte d\n",
            "Texto generado: de la respuesta que de parte d\n",
            "abuerna ventura, que escudiré a\n",
            "el deneos\n",
            "verdos, y los desmostiese grandedamiento y les llefamás.\n",
            "  \n",
            "escriró aponerme desosogrín por mar dentes por pesarias de ser una,\n",
            "viendo a los echozan, si mayo las dube fue de la ceta una alquelan? pero me sute querer herraje, y\n",
            "tan labraldad se respondió la\n",
            "\n",
            "------> Epoch: 9 - Generando texto con temperature 0.5\n",
            "Seed: de la respuesta que de parte d\n",
            "Texto generado: de la respuesta que de parte d angencio caquiéllos queda no\n",
            "vuestra beventeridores ni menestercio sancho, que no es quedo lotro con el reùudo, y dijo?\n",
            "  qiino\n",
            "males toda mentir se ejegenciantos a mi en ata de mis\n",
            "sagiados, pues sasancia su engracía, y sejezó; ¡musped hombre el buen tromedio decirsimacer en los laberosos, como e\n",
            "\n",
            "------> Epoch: 9 - Generando texto con temperature 1.0\n",
            "Seed: de la respuesta que de parte d\n",
            "Texto generado: de la respuesta que de parte d los nos dientes como acadeba, aundo dejado le camentinio don quijote puso que has luego de una en la\n",
            "tierra duquesa, desdijo: mucho, ardizó tiempo sabía ayomo, se conque tanto que quedrígones, y persona parté\n",
            "tu villente:\n",
            ")i decir víe ín ser comuntino, que tanto libro y viero desotro los yísíselo \n",
            "\n",
            "------> Epoch: 9 - Generando texto con temperature 1.2\n",
            "Seed: de la respuesta que de parte d\n",
            "Texto generado: de la respuesta que de parte dscajido silo se\n",
            "otron ristudirle perder\n",
            "despolida, el poco, a esto cempase uno a ella y quizáronle mi preventrarsidad con\n",
            "ellas\n",
            "os traimas que yo me idema que no quedarla\n",
            "ventera; y las tristadas,\n",
            "como tan vía buena fuerca éntes él camisureja de que se budas no se\n",
            "caballero despoldiban la muentaba \n",
            "\n",
            "Epoch 10/10\n",
            "500000/500000 [==============================] - 28s 56us/step - loss: 1.5059 - accuracy: 0.5434\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 10 - Generando texto con temperature 0.2\n",
            "Seed: muchos de nosotros les parecía\n",
            "Texto generado: muchos de nosotros les parecíaos, sin dar.\n",
            "  que necre pies\n",
            "brazo sus\n",
            "señoros, llamaron carrasco; pero. ventible panza, en elomenter, como pera fuera\n",
            "y olvidado a pasabra, viendo, rey vuestra labradora, aunque si sería si esquilar a puede las histores, que está.\n",
            " a tenguo dar a oña pidie ida para de caballero fuera que no quier\n",
            "\n",
            "------> Epoch: 10 - Generando texto con temperature 0.5\n",
            "Seed: muchos de nosotros les parecía\n",
            "Texto generado: muchos de nosotros les parecía los drespandaress parecer?\n",
            "\n",
            "-recimo de que rivenza las delinazos de entramenceras cosas bodos, sugaron a partijo.\n",
            " reventió a entrar cuita, que mé\n",
            "la acuilla que nido valir de mi tal siese quiere amundo a más que sea cita, y, cardó\n",
            "de no de ver se gaala nuus partes y arresaliadas bien verdrarse es\n",
            "\n",
            "------> Epoch: 10 - Generando texto con temperature 1.0\n",
            "Seed: muchos de nosotros les parecía\n",
            "Texto generado: muchos de nosotros les parecía, mudo por\n",
            "sentenso, no cuando le debe lengii no sé sees soer aquella sus\n",
            "fuese del conténseros viede que temer; esta año su guarda,\n",
            "señor el aselevo se señor cuandole del bajel otro que ha de\n",
            "ñoma, que tú pies vorte. vuestra merced, alca la ajente rey el de galanícuntas a su contada. porque su azo\n",
            "\n",
            "------> Epoch: 10 - Generando texto con temperature 1.2\n",
            "Seed: muchos de nosotros les parecía\n",
            "Texto generado: muchos de nosotros les parecía que en podido\n",
            "el mel había estado tus basito de eran si\n",
            "apadiersa. ¿qué\n",
            "hiciera nuestra merced, estaca -respondió sancho, benegoste y otrállo y mayoso dispones cargulas vadas y ir fuero. don los juíos, a hallar con la que escribir:\n",
            "\n",
            "-ango no que yos nocveran y\n",
            "lobrosamiento el carito fuera, en gra\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f726ebfd208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BpEWoL7t40f",
        "colab_type": "text"
      },
      "source": [
        "##############################################################################\n",
        "#################################################################################\n",
        "\n",
        "COMENTARIOS : \n",
        "  Con cada epoca es posible observar como el modelo va mejorando, yo he probado con dos tipos de arquitecturas de red neuronal y he dejando la que mejor resultados presenta, la primera solo con una capa de LSTM y la segunda con dos capas. \n",
        "\n",
        "  LA temperatura es subjetiva a veces es mejor con temperatura de 0.5 y otras veces de 1, pero al final creo que esto esta ajustado a cada epoca.\n",
        "\n",
        "  Otra anotacion importante es el el tamaño de la batch para el entrenamiento, yo he decidio utilizar un tamaño igual a 1024*16 = 16.384‬, que ha sido el que mejor resultados me presenta, asi como tambien me permite optimizar el uso de la GPU, ya que un entrenamiento completo me dura aproxiamadamente 15 minutos en colab. Este valor lo fui probando hasta optener el mejor (desde mi perspectiva).\n",
        "#############################################################################\n",
        "#############################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pBbmz9DMhVhc"
      },
      "source": [
        "## Entregable\n",
        "\n",
        "Completa los apartados anteriores para entrenar modelos del lenguaje que sean capaces de generar texto con cierto sentido. Comentar los resultados obtenidos y cómo el modelo va mejorando época a época. Comentar las diferencias apreciadas al utilizar diferentes valores de temperatura. Entregar al menos la salida de un entrenamiento completo con los textos generados época a época.\n",
        "\n",
        "El objetivo no es conseguir generar pasajes literarios con coherencia, sino obtener lenguaje que se asemeje en cierta manera a lo visto en el texto original y donde las palabras sean reconocibles como construcciones en castellano. Como ejemplo de lo que se puede conseguir, este es el resultado de generar texto después de 10 epochs y con temperature 0.2:\n",
        "\n",
        "\n",
        "```\n",
        "-----> Epoch: 10 - Generando texto con temperature 0.2\n",
        "Seed: o le cautivaron y rindieron el\n",
        "Texto generado: o le cautivaron y rindieron el caballero de la caballería de la mano de la caballería del cual se le dijo:\n",
        "\n",
        "-¿quién es el verdad de la caballería de la caballería de la caballería de la caballería de la caballería, y me ha de habían de la mano que el caballero de la mano de la caballería. y que no se le habían de la mano de la c\n",
        "\n",
        "```\n",
        "\n",
        "Asimismo, se proponen los siguientes aspectos opcionales para conseguir nota extra:\n",
        "\n",
        "*   Experimentar con los textos de teatro en verso de Calderón de la Barca (¿es capaz el modelo de aprender las estructuras del teatro en verso?) o con alguno de los otros textos disponibles. También se puede probar con textos de vuestra elección.\n",
        "*   Experimentar con distintos valores de SEQ_LENGTH.\n",
        "*   Experimentar con los hiperparámetros del modelo o probar otro tipo de modelos como GRUs o *stacked* RNNs (RNNs apiladas).\n",
        "*   Experimentar utilizando embeddings en vez de representaciones one-hot.\n",
        "*   (Difícil) Entrenar un modelo secuencia a secuencia en vez de secuencia a carácter.\n",
        "\n",
        "\n"
      ]
    }
  ]
}